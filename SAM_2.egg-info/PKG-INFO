Metadata-Version: 2.4
Name: SAM-2
Version: 1.0
Summary: SAM 2: Segment Anything in Images and Videos
Home-page: https://github.com/facebookresearch/sam2
Author: Meta AI
Author-email: segment-anything@meta.com
License: Apache 2.0
Requires-Python: >=3.10.0
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: LICENSE_cctorch
Requires-Dist: torch>=2.3.1
Requires-Dist: torchvision>=0.18.1
Requires-Dist: numpy>=1.24.4
Requires-Dist: tqdm>=4.66.1
Requires-Dist: hydra-core>=1.3.2
Requires-Dist: iopath>=0.1.10
Requires-Dist: pillow>=9.4.0
Provides-Extra: notebooks
Requires-Dist: matplotlib>=3.9.1; extra == "notebooks"
Requires-Dist: jupyter>=1.0.0; extra == "notebooks"
Requires-Dist: opencv-python>=4.7.0; extra == "notebooks"
Requires-Dist: eva-decord>=0.6.1; extra == "notebooks"
Provides-Extra: interactive-demo
Requires-Dist: Flask>=3.0.3; extra == "interactive-demo"
Requires-Dist: Flask-Cors>=5.0.0; extra == "interactive-demo"
Requires-Dist: av>=13.0.0; extra == "interactive-demo"
Requires-Dist: dataclasses-json>=0.6.7; extra == "interactive-demo"
Requires-Dist: eva-decord>=0.6.1; extra == "interactive-demo"
Requires-Dist: gunicorn>=23.0.0; extra == "interactive-demo"
Requires-Dist: imagesize>=1.4.1; extra == "interactive-demo"
Requires-Dist: pycocotools>=2.0.8; extra == "interactive-demo"
Requires-Dist: strawberry-graphql>=0.243.0; extra == "interactive-demo"
Provides-Extra: dev
Requires-Dist: black==24.2.0; extra == "dev"
Requires-Dist: usort==1.0.2; extra == "dev"
Requires-Dist: ufmt==2.0.0b2; extra == "dev"
Requires-Dist: fvcore>=0.1.5.post20221221; extra == "dev"
Requires-Dist: pandas>=2.2.2; extra == "dev"
Requires-Dist: scikit-image>=0.24.0; extra == "dev"
Requires-Dist: tensorboard>=2.17.0; extra == "dev"
Requires-Dist: pycocotools>=2.0.8; extra == "dev"
Requires-Dist: tensordict>=0.5.0; extra == "dev"
Requires-Dist: opencv-python>=4.7.0; extra == "dev"
Requires-Dist: submitit>=1.5.1; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# SAM2 Real-Time Detection and Tracking

This repository adapts **[SAM2](https://github.com/facebookresearch/sam2)** to include real-time multi-object tracking. 
It allows users to specify and track a fixed number of objects in real time, integrating motion modeling 
from **[SAMURAI](https://github.com/yangchris11/samurai)** for improved tracking in complex scenarios.  

### About SAM2
**SAM2** (Segment Anything Model 2) is designed for object segmentation and tracking but lacks built-in capabilities 
for performing this in real time.

### About SAMURAI
**SAMURAI** enhances SAM2 by introducing motion modeling, leveraging temporal motion cues for better 
tracking accuracy without retraining or fine-tuning.  

#### Note on SAMURAI Features
While this repository integrates SAMURAI's motion modeling, it does not support the motion-aware memory selection mechanism, 
as conditional operations applied to memory would simultaneously affect all tracked objects.

The core implementation of SAMURAI's motion modeling, originally found in 
[sam2_base.py](https://github.com/yangchris11/samurai/blob/master/sam2/sam2/modeling/sam2_base.py) in SAMURAI's 
repository, has been relocated to 
[sam2_object_tracker.py](https://github.com/zdata-inc/sam2_realtime/blob/main/sam2/sam2_object_tracker.py) in this 
repository to maintain the original SAM2 codebase.

## Key Features

- **Real-Time Tracking**: Modified SAM2 to track a fixed number of objects in real time.
- **Motion-Aware Memory**: SAMURAI leverages temporal motion cues for robust object tracking without retraining or fine-tuning.

The core implementation resides in `sam2_object_tracker.py`, where the number of objects to track must be specified during instantiation.

---

## Setup Instructions

### 1. Create Conda Environment
```bash
conda create -n sam2 python=3.10.15
```

### 2. Clone SAM2 Realtime Repo
```
git clone https://github.com/zdata-inc/sam2_realtime
```

### 2. Install SAM2
```
cd sam2_realtime
pip install -e .
pip install -e ".[notebooks]"
```


### 4. Download SAM2 Checkpoints
```bash
cd checkpoints
./download_ckpts.sh
cd ..
```

---

## Usage
### Demo
Run the demo notebook to visualize real-time SAM2 object tracking in action:  
`notebooks/realtime_detect_and_track.ipynb`


### Acknowledgment
SAM2 Realtime extends [SAM 2](https://github.com/facebookresearch/sam2) by Meta FAIR, designed to enable real-time tracking. 
It integrates motion-aware segmentation techniques developed in [SAMURAI](https://github.com/yangchris11/samurai) by 
the Information Processing Lab at the University of Washington.


## Citation
```
@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, 
  Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, 
  Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, 
  Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}

@misc{yang2024samurai,
      title={SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory}, 
      author={Cheng-Yen Yang and Hsiang-Wei Huang and Wenhao Chai and Zhongyu Jiang and Jenq-Neng Hwang},
      year={2024},
      eprint={2411.11922},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.11922}, 
}
```

---
